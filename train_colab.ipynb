{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobile LLM Lab - Google Colab Training\n",
    "\n",
    "This notebook trains Hugging Face models using free GPU resources from Google Colab.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. **Enable GPU**: Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\n",
    "2. **Set Secrets**: Go to the key icon (üîë) on the left sidebar:\n",
    "   - Add `HF_TOKEN` with your Hugging Face token\n",
    "   - Add `GH_TOKEN` with your GitHub token (optional, for pushing results)\n",
    "   - Add `HF_USERNAME` with your Hugging Face username\n",
    "3. **Run All Cells**: Runtime ‚Üí Run all\n",
    "\n",
    "The notebook will automatically:\n",
    "- Clone your GitHub repo\n",
    "- Install dependencies\n",
    "- Train the model\n",
    "- Push results to Hugging Face Hub\n",
    "- Commit checkpoints back to GitHub (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Edit these variables to customize your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "GITHUB_REPO = \"YOUR_GITHUB_USERNAME/mobile-llm-lab\"  # Change this!\n",
    "MODEL_NAME = \"assistant_v1\"  # Name for your fine-tuned model\n",
    "BASE_MODEL = \"distilbert-base-uncased\"  # Base model from Hugging Face\n",
    "DATASET = \"dataset/mydata.txt\"  # Path to dataset in repo\n",
    "TASK_TYPE = \"causal_lm\"  # causal_lm or classification\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Set to True to push model to Hugging Face Hub\n",
    "PUSH_TO_HUB = True\n",
    "\n",
    "# Set to True to commit checkpoints back to GitHub\n",
    "PUSH_TO_GITHUB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected. Training will be slow!\")\n",
    "    print(\"Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Load tokens from Colab secrets\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"‚úì HF_TOKEN loaded\")\n",
    "except:\n",
    "    print(\"‚ùå HF_TOKEN not found! Add it in Secrets (üîë icon on left)\")\n",
    "    HF_TOKEN = None\n",
    "\n",
    "try:\n",
    "    HF_USERNAME = userdata.get('HF_USERNAME')\n",
    "    print(f\"‚úì HF_USERNAME loaded: {HF_USERNAME}\")\n",
    "except:\n",
    "    print(\"‚ùå HF_USERNAME not found! Add it in Secrets\")\n",
    "    HF_USERNAME = None\n",
    "\n",
    "try:\n",
    "    GH_TOKEN = userdata.get('GH_TOKEN')\n",
    "    print(\"‚úì GH_TOKEN loaded\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è GH_TOKEN not found (optional, only needed for private repos or pushing back)\")\n",
    "    GH_TOKEN = None\n",
    "\n",
    "# Set environment variables\n",
    "if HF_TOKEN:\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "if GH_TOKEN:\n",
    "    os.environ['GH_TOKEN'] = GH_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove existing repo if present\n",
    "!rm -rf mobile-llm-lab\n",
    "\n",
    "# Clone repo (use token for private repos)\n",
    "if GH_TOKEN:\n",
    "    repo_url = f\"https://{GH_TOKEN}@github.com/{GITHUB_REPO}.git\"\n",
    "else:\n",
    "    repo_url = f\"https://github.com/{GITHUB_REPO}.git\"\n",
    "\n",
    "!git clone {repo_url} mobile-llm-lab\n",
    "%cd mobile-llm-lab\n",
    "\n",
    "# Configure git for commits (if pushing back)\n",
    "!git config user.email \"colab@training.ai\"\n",
    "!git config user.name \"Colab Training Bot\"\n",
    "\n",
    "print(\"\\n‚úì Repository cloned successfully\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install -q transformers datasets torch accelerate huggingface_hub\n",
    "\n",
    "print(\"\\n‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training command\n",
    "cmd = f\"\"\"python train.py \\\n",
    "    --model_name {MODEL_NAME} \\\n",
    "    --base_model {BASE_MODEL} \\\n",
    "    --dataset {DATASET} \\\n",
    "    --task_type {TASK_TYPE} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --batch_size {BATCH_SIZE} \\\n",
    "    --learning_rate {LEARNING_RATE} \\\n",
    "    --max_length {MAX_LENGTH} \\\n",
    "    --hf_username {HF_USERNAME}\"\"\"\n",
    "\n",
    "if PUSH_TO_HUB:\n",
    "    cmd += \" --push_to_hub\"\n",
    "\n",
    "print(\"Starting training with command:\")\n",
    "print(cmd)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Run training\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Results to GitHub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PUSH_TO_GITHUB and GH_TOKEN:\n",
    "    print(\"Committing and pushing results to GitHub...\")\n",
    "    \n",
    "    # Add model files\n",
    "    !git add models/{MODEL_NAME}\n",
    "    \n",
    "    # Commit\n",
    "    commit_msg = f\"Training completed for {MODEL_NAME}\"\n",
    "    !git commit -m \"{commit_msg}\"\n",
    "    \n",
    "    # Push\n",
    "    !git push\n",
    "    \n",
    "    print(\"‚úì Results pushed to GitHub\")\n",
    "elif PUSH_TO_GITHUB:\n",
    "    print(\"‚ö†Ô∏è Cannot push to GitHub: GH_TOKEN not set\")\n",
    "else:\n",
    "    print(\"Skipping GitHub push (PUSH_TO_GITHUB = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if training completed\n",
    "model_dir = Path(f\"models/{MODEL_NAME}\")\n",
    "if model_dir.exists():\n",
    "    print(f\"‚úì Model saved: models/{MODEL_NAME}\")\n",
    "    \n",
    "    # Load metrics if available\n",
    "    metrics_file = model_dir / \"training_metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file) as f:\n",
    "            metrics = json.load(f)\n",
    "        print(\"\\nTraining Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # List saved files\n",
    "    print(\"\\nSaved Files:\")\n",
    "    for file in sorted(model_dir.glob(\"*\")):\n",
    "        size = file.stat().st_size / (1024*1024)  # MB\n",
    "        print(f\"  {file.name} ({size:.2f} MB)\")\n",
    "    \n",
    "    if PUSH_TO_HUB and HF_USERNAME:\n",
    "        print(f\"\\n‚úì Model pushed to: https://huggingface.co/{HF_USERNAME}/{MODEL_NAME}\")\n",
    "else:\n",
    "    print(\"‚ùå Model directory not found. Training may have failed.\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
